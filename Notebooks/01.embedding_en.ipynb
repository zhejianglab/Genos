{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Biological Sequence Embedding Extraction\n",
        "\n",
        "## What is Embedding?\n",
        "\n",
        "**Embedding (Embedding Vector)** is a technique for converting text, sequences, or other unstructured data into numerical vectors. In bioinformatics, embedding can convert biological sequences such as DNA sequences and protein sequences into high-dimensional numerical vectors. These vectors can:\n",
        "\n",
        "1. **Capture semantic information of sequences**: Similar sequences produce similar vectors\n",
        "2. **Support machine learning**: Numerical vectors can be directly used in various machine learning algorithms\n",
        "3. **Dimensional reduction representation**: Compress complex sequence information into fixed-length vectors\n",
        "4. **Calculate similarity**: Calculate similarity between sequences through vector distance\n",
        "\n",
        "## Why Extract Embedding?\n",
        "\n",
        "In bioinformatics research, embedding extraction has important value:\n",
        "\n",
        "- **Sequence classification**: Identify functional types of DNA sequences (such as promoters, enhancers, etc.)\n",
        "- **Sequence similarity analysis**: Quickly find similar biological sequences\n",
        "- **Functional prediction**: Predict protein function based on sequence embedding\n",
        "- **Evolutionary analysis**: Study evolutionary relationships of sequences\n",
        "\n",
        "## This tutorial will demonstrate:\n",
        "\n",
        "1. How to load pre-trained [Genos-1.2B](https://huggingface.co/BGI-HangzhouAI/Genos-1.2B) or [Genos-10B](https://huggingface.co/BGI-HangzhouAI/Genos-10B) sequence models\n",
        "2. How to convert DNA sequences into embedding vectors\n",
        "3. How to analyze embedding features from different layers\n",
        "4. Understanding the meaning and applications of embedding vectors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Required Libraries\n",
        "\n",
        "First, we need to import the Python libraries required for processing embeddings:\n",
        "\n",
        "- **torch**: PyTorch deep learning framework for model inference\n",
        "- **transformers**: Hugging Face's transformers library, providing pre-trained models and tokenizers\n",
        "  - **AutoModel**: Automatically load pre-trained models\n",
        "  - **AutoTokenizer**: Automatically load corresponding tokenizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Pre-trained Model\n",
        "\n",
        "Here we load a pre-trained model specifically designed for biological sequences:\n",
        "\n",
        "- **model_path**: Specify the path to the pre-trained model (here using a biological sequence-specific model)\n",
        "- **tokenizer**: Tokenizer responsible for converting DNA sequences into tokens that the model can understand\n",
        "- **model**: Pre-trained model, set `output_hidden_states=True` to get hidden states from all layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify model path\n",
        "model_path = \"/path/to/your/local/Genos-1.2B\" # Replace with local model path\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Load locally downloaded Genos weights\n",
        "model = AutoModel.from_pretrained(model_path, \n",
        "                                  output_hidden_states=True,\n",
        "                                  torch_dtype=torch.bfloat16, \n",
        "                                  trust_remote_code=True, \n",
        "                                  attn_implementation=\"flash_attention_2\" # Use flash_attention\n",
        "  )\n",
        "model.cuda() # Load model to GPU\n",
        "model.eval() # Switch model to inference mode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Prepare Input Sequence\n",
        "\n",
        "We create a DNA sequence as an example:\n",
        "\n",
        "- **text**: Randomly generated sequence of specified length\n",
        "- This sequence contains common DNA bases (A, T, G, C)\n",
        "\n",
        "**In practical applications**, you can replace it with any real DNA sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Randomly select a specific number of bases\n",
        "bases = ['A', 'T', 'G', 'C']\n",
        "seqs = random.choices(bases, k=8192)\n",
        "# Generate input base sequence\n",
        "text = ''.join(seqs)\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Sequence Encoding\n",
        "\n",
        "Use the tokenizer to convert DNA sequences into a format that the model can process:\n",
        "\n",
        "- **tokenizer(text, return_tensors=\"pt\")**: Convert text sequence to PyTorch tensor\n",
        "- **return_tensors=\"pt\"**: Return PyTorch format tensor\n",
        "- **inputs**: Contains encoded sequence information, including input_ids, attention_mask, etc.\n",
        "\n",
        "This step converts the original DNA string into a digitized token sequence, where each base or base combination corresponds to a unique token ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# View encoded token sequence\n",
        "print(inputs['input_ids'])\n",
        "\n",
        "# Load data to GPU\n",
        "inputs = {k: v.cuda() for k, v in inputs.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Model Inference\n",
        "\n",
        "Perform forward propagation through the pre-trained model to obtain embeddings:\n",
        "\n",
        "- **torch.no_grad()**: Disable gradient computation since we only need inference, not training\n",
        "- **model(**inputs)**: Input encoded sequence into the model\n",
        "- **outputs**: Model output, containing logits from the last layer and hidden states from all layers\n",
        "\n",
        "This step is the core of embedding extraction, where the model converts sequences into high-dimensional vector representations based on pre-trained knowledge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Extract Layer-wise Embeddings\n",
        "\n",
        "Obtain hidden states (embeddings) from each layer of the model:\n",
        "\n",
        "- **outputs.hidden_states**: Contains hidden states from all layers, is a tuple\n",
        "- **hidden_states[i]**: Embedding vector from layer i\n",
        "- **shape**: Each embedding has shape [batch_size, sequence_length, hidden_size]\n",
        "\n",
        "**Important Note**:\n",
        "- Embeddings from different layers capture semantic information at different levels\n",
        "- Shallow layers typically capture local features (such as base patterns)\n",
        "- Deep layers capture more abstract semantic information (such as functional domains, structural features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get hidden states from all layers\n",
        "hidden_states = outputs.hidden_states  # Tuple containing hidden states from each layer\n",
        "\n",
        "# Iterate through embeddings from each layer and display key information\n",
        "for i, layer_embedding in enumerate(hidden_states):\n",
        "    print(f\"Layer {i} embedding ({layer_embedding.shape}): {layer_embedding[0, 0, :10]}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Using Genos Package to Directly Get Embeddings\n",
        "\n",
        "\n",
        "Note: Due to current limited resources, the API currently provides models supporting 1.2B and 10B, with a maximum embedding length of **128k**, and only returns embeddings from the last layer\n",
        "\n",
        "- Parameter description\n",
        "    - sequence, sequence, maximum length not exceeding 128k\n",
        "    - model_name, model name, \"Genos-1.2B\" or \"Genos-10B\"\n",
        "    - pooling_method, pooling method, \"mean\": mean pooling, \"max\": max pooling, \"min\": min pooling, \"last\": take embedding of the last token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from genos import create_client\n",
        "\n",
        "client = create_client(token=\"<your_api_key>\")\n",
        "\n",
        "result = client.get_embedding(sequence=text, model_name=\"Genos-1.2B\", pooling_method=\"mean\")\n",
        "print(result['result']['embedding'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Applications of Embeddings\n",
        "\n",
        "Extracted embeddings can be used for:\n",
        "\n",
        "1. **Sequence similarity calculation**:\n",
        "   ```python\n",
        "   # Calculate cosine similarity between two sequences\n",
        "   similarity = cosine_similarity(embedding1, embedding2)\n",
        "   ```\n",
        "\n",
        "2. **Sequence classification**:\n",
        "   ```python\n",
        "   # Train classifier using embeddings\n",
        "   classifier = train_classifier(embeddings, labels)\n",
        "   ```\n",
        "\n",
        "3. **Clustering analysis**:\n",
        "   ```python\n",
        "   # Perform clustering on sequences\n",
        "   clusters = kmeans_clustering(embeddings)\n",
        "   ```\n",
        "\n",
        "4. **Dimensional reduction visualization**:\n",
        "   ```python\n",
        "   # Use t-SNE or PCA for dimensional reduction visualization\n",
        "   reduced_embeddings = tsne.fit_transform(embeddings)\n",
        "   ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Through this tutorial, we learned:\n",
        "\n",
        "### Core Concepts\n",
        "- **Embedding**: Technique for converting biological sequences into numerical vectors\n",
        "- **Pre-trained models**: Large language models specifically trained for biological sequences\n",
        "- **Hierarchical features**: Different layers capture sequence information at different levels\n",
        "\n",
        "### Technical Process\n",
        "1. Load pre-trained biological sequence models\n",
        "2. Encode DNA sequences into tokens\n",
        "3. Obtain embeddings through model inference\n",
        "4. Analyze feature representations from different layers\n",
        "\n",
        "### Practical Value\n",
        "- **Accelerate research**: Quickly analyze large amounts of biological sequences\n",
        "- **Improve accuracy**: Use pre-trained knowledge to enhance prediction performance\n",
        "- **Support downstream tasks**: Provide foundation for classification, clustering, similarity analysis, etc.\n",
        "\n",
        "### Next Examples\n",
        "1. Use extracted embeddings for downstream population prediction tasks\n",
        "2. Use extracted embeddings for downstream variant prediction tasks\n",
        "3. RNA seq prediction\n",
        "\n",
        "\n",
        "**Congratulations!** You have mastered the basic methods of biological sequence embedding extraction!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
